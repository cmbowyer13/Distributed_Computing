{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is to start playing around with PySpark and start speeding up code using distributed computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, a PySpark program isn’t that much different from a regular Python program, but the execution model can be very different from a regular Python program, especially if you’re running on a cluster.\n",
    "\n",
    "There can be a lot of things happening behind the scenes that distribute the processing across multiple nodes if you’re on a cluster. However, for now, think of the program as a Python program that uses the PySpark library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark documentation: https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark installationn:\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/install.html#python-version-supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Python context, think of PySpark has a way to handle parallel processing without the need for the threading or multiprocessing modules. All of the complicated communication and synchronization between threads, processes, and even different CPUs is handled by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with PySpark, you create specialized data structures called Resilient Distributed Datasets (RDDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, you’ll run PySpark programs on a Hadoop cluster, but other cluster deployment options are supported. You can read Spark’s cluster mode overview for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command: \"pip install pyspark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have to write some variables in .bashrc file:\n",
    "# https://stackoverflow.com/questions/30518362/how-do-i-set-the-drivers-python-version-in-spark\n",
    "\n",
    "# https://sparkbyexamples.com/spark/spark-exception-python-in-worker-has-different-version-3-4-than-that-in-driver-2-7-pyspark-cannot-run-with-different-minor-versions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Hello World:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# steps for Pyspark program, use !which python and set env vars below: \n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/Users/calebbowyer/anaconda3/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/Users/calebbowyer/anaconda3/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test speedups of distributed computing from using various numbers of cpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "3\n",
      "CPU times: user 47.8 ms, sys: 30.2 ms, total: 78 ms\n",
      "Wall time: 9.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sc = pyspark.SparkContext('local[*]')\n",
    "sc = pyspark.SparkContext('local[1]')\n",
    "txt = sc.textFile('distributed_computing_notes.txt')\n",
    "# print(txt)\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(python_lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry-point of any PySpark program is a SparkContext object. This object allows you to connect to a Spark cluster and create RDDs. The local[*] string is a special string denoting that you’re using a local cluster, which is another way of saying you’re running in single-machine mode. The * tells Spark to create as many worker threads as logical cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "3\n",
      "CPU times: user 24.4 ms, sys: 10.1 ms, total: 34.4 ms\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sc = pyspark.SparkContext('local[*]')\n",
    "sc2 = pyspark.SparkContext('local[2]')\n",
    "txt = sc2.textFile('distributed_computing_notes.txt')\n",
    "# print(txt)\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(python_lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "3\n",
      "CPU times: user 21.8 ms, sys: 6.87 ms, total: 28.6 ms\n",
      "Wall time: 679 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sc = pyspark.SparkContext('local[*]')\n",
    "sc3 = pyspark.SparkContext('local[3]')\n",
    "txt = sc3.textFile('distributed_computing_notes.txt')\n",
    "# print(txt)\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(python_lines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "3\n",
      "CPU times: user 22.4 ms, sys: 7.59 ms, total: 30 ms\n",
      "Wall time: 737 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sc = pyspark.SparkContext('local[*]')\n",
    "sc4 = pyspark.SparkContext('local[4]')\n",
    "txt = sc4.textFile('distributed_computing_notes.txt')\n",
    "# print(txt)\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(python_lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note that for the size of this problem using 3 CPU threads was faster than using 4, so we can stop here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
